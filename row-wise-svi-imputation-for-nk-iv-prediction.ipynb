{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-02T17:59:21.884316Z",
     "iopub.status.busy": "2025-06-02T17:59:21.883983Z",
     "iopub.status.idle": "2025-06-02T17:59:21.891033Z",
     "shell.execute_reply": "2025-06-02T17:59:21.890040Z",
     "shell.execute_reply.started": "2025-06-02T17:59:21.884291Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# 0. Imports & helpers\n",
    "# ===============================================================\n",
    "\n",
    "import numpy as np, pandas as pd, scipy.optimize as opt, math, gc, pickle, pathlib\n",
    "from tqdm.auto import tqdm\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "pathlib.Path(\"cache\").mkdir(exist_ok=True)\n",
    "\n",
    "# ---------- SVI functions ----------\n",
    "def svi_total_variance(p, k):\n",
    "    a, b, rho, m, sig = p\n",
    "    return a + b * (rho * (k - m) + np.sqrt((k - m) ** 2 + sig ** 2))\n",
    "\n",
    "def svi_obj(p, k_obs, iv_obs):\n",
    "    w = svi_total_variance(p, k_obs)\n",
    "    if (w <= 0).any(): \n",
    "        return np.inf\n",
    "    return ((np.sqrt(w) - iv_obs) ** 2).sum()\n",
    "\n",
    "def calibrate_svi(strikes, ivs, fpx):\n",
    "    if len(strikes) < 5:               # not enough pts\n",
    "        return None\n",
    "    k = np.log(strikes / fpx)\n",
    "    x0 = np.array([ max(ivs.min()**2*0.8,1e-6), 0.1, 0.0, 0.0, 0.1 ])\n",
    "    bnds = [(1e-8,np.inf),(1e-8,np.inf),(-.999,.999),(k.min()-.5,k.max()+.5),(1e-8,5)]\n",
    "    try:\n",
    "        res = opt.minimize(svi_obj, x0, args=(k, ivs), bounds=bnds,\n",
    "                           method=\"L-BFGS-B\",\n",
    "                           options={\"maxiter\":2000,\"ftol\":1e-12})\n",
    "        return None if (not res.success) else res.x\n",
    "    except Exception:                  # failed fit\n",
    "        return None\n",
    "\n",
    "def svi_iv(p, strikes, fpx):\n",
    "    if p is None: \n",
    "        return np.full(len(strikes), np.nan)\n",
    "    k = np.log(strikes / fpx)\n",
    "    w = np.maximum(svi_total_variance(p, k), 1e-14)\n",
    "    return np.sqrt(w)\n",
    "\n",
    "# ---------- 30-min block helper ----------\n",
    "def assign_blocks(ts_ns, block_sec=30*60):\n",
    "    ts_s = (ts_ns.astype(np.int64) // 10**9).astype(int)\n",
    "    return (ts_s // block_sec).astype(int)\n",
    "\n",
    "# ===============================================================\n",
    "# 1. Melt wide → long   (cached)\n",
    "# ===============================================================\n",
    "\n",
    "if (p := pathlib.Path(\"cache/train_long_raw.parquet\")).exists():\n",
    "    train_long = pd.read_parquet(p)\n",
    "    print(\"✓ loaded melted train_long from cache.\")\n",
    "else:\n",
    "    train_wide = pd.read_parquet(\"/kaggle/input/nk-iv-prediction/train_data.parquet\")\n",
    "    iv_cols = [c for c in train_wide.columns if c.startswith((\"call_iv_\",\"put_iv_\"))]\n",
    "    calls = [c for c in iv_cols if c.startswith(\"call_iv_\")]\n",
    "    puts  = [c for c in iv_cols if c.startswith(\"put_iv_\")]\n",
    "\n",
    "    def melt(df, cols, cp_flag, prefix):\n",
    "        tmp = df.melt(\n",
    "            id_vars=[\"timestamp\",\"underlying\",\"expiry\"]+[f\"X{i}\" for i in range(42)],\n",
    "            value_vars=cols, var_name=\"var\", value_name=\"iv\"\n",
    "        )\n",
    "        tmp[\"cp_flag\"] = cp_flag\n",
    "        tmp[\"strike\"]  = tmp[\"var\"].str.replace(prefix,\"\").astype(int)\n",
    "        return tmp.drop(columns=\"var\")\n",
    "\n",
    "    train_long = pd.concat(\n",
    "        [melt(train_wide, calls, 0,\"call_iv_\"),\n",
    "         melt(train_wide, puts , 1,\"put_iv_\")],\n",
    "        ignore_index=True\n",
    "    )\n",
    "    train_long.to_parquet(p, index=False)\n",
    "    print(\"✓ melted train_long written to cache/train_long_raw.parquet\")\n",
    "\n",
    "# ===============================================================\n",
    "# 2. SVI calibration per snapshot  (cached)\n",
    "# ===============================================================\n",
    "\n",
    "param_file = pathlib.Path(\"cache/svi_params.pkl\")\n",
    "svi_file   = pathlib.Path(\"cache/train_with_svi.parquet\")\n",
    "\n",
    "if svi_file.exists() and param_file.exists():\n",
    "    train_long = pd.read_parquet(svi_file)\n",
    "    params_dict = pickle.loads(param_file.read_bytes())\n",
    "    print(\"✓ loaded iv_svi & params from cache.\")\n",
    "else:\n",
    "    params_dict = {}\n",
    "    train_long[\"iv_svi\"] = np.nan\n",
    "\n",
    "    grp_cols = [\"timestamp\",\"expiry\",\"underlying\"]\n",
    "    for key, sub in tqdm(train_long.groupby(grp_cols, sort=False),\n",
    "                         total=train_long.groupby(grp_cols).ngroups,\n",
    "                         desc=\"SVI fits\"):\n",
    "        p = calibrate_svi(sub[\"strike\"].values[~sub[\"iv\"].isna()],\n",
    "                          sub[\"iv\"].values[~sub[\"iv\"].isna()],\n",
    "                          key[2])\n",
    "        params_dict[key] = p\n",
    "        train_long.loc[sub.index, \"iv_svi\"] = svi_iv(p, sub[\"strike\"].values, key[2])\n",
    "\n",
    "    train_long.to_parquet(svi_file, index=False)\n",
    "    param_file.write_bytes(pickle.dumps(params_dict))\n",
    "    print(\"✓ iv_svi + params cached to disk.\")\n",
    "\n",
    "# ===============================================================\n",
    "# 3. Feature engineering   (cheap, no need to cache)\n",
    "# ===============================================================\n",
    "\n",
    "train_long[\"log_mny\"]  = np.log(train_long[\"strike\"] / train_long[\"underlying\"])\n",
    "train_long[\"abs_mny\"]  = train_long[\"log_mny\"].abs()\n",
    "train_long[\"log_mny_sq\"] = train_long[\"log_mny\"]**2\n",
    "train_long[\"log_mny_cu\"] = train_long[\"log_mny\"]**3\n",
    "train_long[\"abs_strike_minus_underlying\"] = (train_long[\"strike\"] - train_long[\"underlying\"]).abs()\n",
    "train_long[\"cpflag_x_absmny\"] = train_long[\"cp_flag\"] * train_long[\"abs_mny\"]\n",
    "\n",
    "base_feats = [\n",
    "    \"log_mny\",\"abs_mny\",\"log_mny_sq\",\"log_mny_cu\",\n",
    "    \"abs_strike_minus_underlying\",\"cpflag_x_absmny\",\n",
    "    \"iv_svi\",\"cp_flag\"\n",
    "] + [f\"X{i}\" for i in range(42)]\n",
    "\n",
    "train_fit = train_long[train_long[\"iv\"].notna()].reset_index(drop=True)\n",
    "train_fit[\"residual\"] = train_fit[\"iv\"] - train_fit[\"iv_svi\"]\n",
    "\n",
    "# ===============================================================\n",
    "# 4. CatBoost 5-fold CV  (cached)\n",
    "# ===============================================================\n",
    "\n",
    "oof_file = pathlib.Path(\"cache/oof_cb.npy\")\n",
    "model_dir = pathlib.Path(\"cache/cat_models\")\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "if oof_file.exists() and all((model_dir/f\"fold{i}.cbm\").exists() for i in range(5)):\n",
    "    oof_cb = np.load(oof_file)\n",
    "    cat_models = [CatBoostRegressor().load_model(model_dir/f\"fold{i}.cbm\") for i in range(5)]\n",
    "    print(\"✓ CatBoost OOF & models loaded from cache.\")\n",
    "else:\n",
    "    cat_models, oof_cb = [], np.zeros(len(train_fit))\n",
    "    train_fit[\"block_id\"] = assign_blocks(train_fit[\"timestamp\"])\n",
    "    blocks = np.sort(train_fit[\"block_id\"].unique())\n",
    "    nfold, step = 5, math.ceil(len(blocks)/5)\n",
    "\n",
    "    for f in range(nfold):\n",
    "        val_blocks = blocks[f*step:(f+1)*step]\n",
    "        val_mask   = train_fit[\"block_id\"].isin(val_blocks)\n",
    "\n",
    "        X_tr, y_tr = train_fit.loc[~val_mask, base_feats], train_fit.loc[~val_mask,\"residual\"]\n",
    "        X_va, y_va = train_fit.loc[ val_mask, base_feats], train_fit.loc[ val_mask,\"residual\"]\n",
    "        X_tr[\"cp_flag\"] = X_tr[\"cp_flag\"].astype(int)\n",
    "        X_va[\"cp_flag\"] = X_va[\"cp_flag\"].astype(int)\n",
    "\n",
    "        cb = CatBoostRegressor(\n",
    "            iterations=2000, depth=6, learning_rate=0.05, l2_leaf_reg=3.0,\n",
    "            eval_metric=\"RMSE\", task_type=\"GPU\", random_seed=42,\n",
    "            early_stopping_rounds=200, verbose=200)\n",
    "        cb.fit(Pool(X_tr,y_tr,cat_features=[\"cp_flag\"]),\n",
    "               eval_set=Pool(X_va,y_va,cat_features=[\"cp_flag\"]))\n",
    "\n",
    "        oof_cb[val_mask.values] = cb.predict(X_va)\n",
    "        cb.save_model(model_dir/f\"fold{f}.cbm\")\n",
    "        cat_models.append(cb)\n",
    "        gc.collect()\n",
    "\n",
    "    np.save(oof_file, oof_cb)\n",
    "    print(\"✓ CatBoost OOF RMSE:\", mean_squared_error(train_fit[\"residual\"], oof_cb, squared=False))\n",
    "\n",
    "# ===============================================================\n",
    "# 5. LightGBM stack  (cached)\n",
    "# ===============================================================\n",
    "\n",
    "oof_lgb_file  = pathlib.Path(\"cache/oof_lgb.npy\")\n",
    "lgb_dir       = pathlib.Path(\"cache/lgb_models\")\n",
    "lgb_dir.mkdir(exist_ok=True)\n",
    "\n",
    "if oof_lgb_file.exists() and all((lgb_dir/f\"fold{i}.txt\").exists() for i in range(5)):\n",
    "    oof_lgb   = np.load(oof_lgb_file)\n",
    "    lgb_models = [lgb.Booster(model_file=str(lgb_dir/f\"fold{i}.txt\")) for i in range(5)]\n",
    "    print(\"✓ LightGBM OOF & models loaded from cache.\")\n",
    "else:\n",
    "    train_fit[\"Z_cb\"] = oof_cb\n",
    "    lgb_feats = base_feats + [\"Z_cb\"]\n",
    "\n",
    "    oof_lgb, lgb_models = np.zeros(len(train_fit)), []\n",
    "    blocks = np.sort(train_fit[\"block_id\"].unique())\n",
    "    nfold, step = 5, math.ceil(len(blocks)/5)\n",
    "\n",
    "    for f in range(nfold):\n",
    "        val_blocks = blocks[f*step:(f+1)*step]\n",
    "        val_mask   = train_fit[\"block_id\"].isin(val_blocks)\n",
    "\n",
    "        X_tr, y_tr = train_fit.loc[~val_mask, lgb_feats], train_fit.loc[~val_mask,\"residual\"]\n",
    "        X_va, y_va = train_fit.loc[ val_mask, lgb_feats], train_fit.loc[ val_mask,\"residual\"]\n",
    "        for c in [\"cp_flag\"]:                   # cast cat\n",
    "            X_tr[c] = X_tr[c].astype(int); X_va[c] = X_va[c].astype(int)\n",
    "\n",
    "        dtr = lgb.Dataset(X_tr, y_tr, categorical_feature=[\"cp_flag\"])\n",
    "        dva = lgb.Dataset(X_va, y_va, categorical_feature=[\"cp_flag\"])\n",
    "\n",
    "        params = dict(objective=\"regression\", metric=\"rmse\",\n",
    "                      boosting=\"gbdt\", learning_rate=0.03,\n",
    "                      num_leaves=64, feature_fraction=0.8,\n",
    "                      bagging_fraction=0.8, bagging_freq=5,\n",
    "                      lambda_l1=0.5, lambda_l2=1.0, seed=42)\n",
    "        bst = lgb.train(params, dtr, 5000, [dva], verbose_eval=200,\n",
    "                        early_stopping_rounds=200)\n",
    "        oof_lgb[val_mask.values] = bst.predict(X_va, num_iteration=bst.best_iteration)\n",
    "        bst.save_model(lgb_dir/f\"fold{f}.txt\")\n",
    "        lgb_models.append(bst)\n",
    "\n",
    "    np.save(oof_lgb_file, oof_lgb)\n",
    "    print(\"✓ CatBoost+LightGBM OOF RMSE:\",\n",
    "          mean_squared_error(train_fit[\"residual\"], oof_lgb, squared=False))\n",
    "\n",
    "# ===============================================================\n",
    "# 6. Inference helper (uses cached models)\n",
    "# ===============================================================\n",
    "\n",
    "def predict_iv_for_test(test_parquet, out_csv):\n",
    "    print(\">>> inference …\")\n",
    "    test_wide = pd.read_parquet(test_parquet)\n",
    "\n",
    "    # ----- melt -----\n",
    "    melt = lambda df, cols, flag, prefix: df.melt(\n",
    "        id_vars=[\"timestamp\",\"underlying\",\"expiry\"]+[f\"X{i}\" for i in range(42)],\n",
    "        value_vars=cols, var_name=\"var\", value_name=\"dummy\"\n",
    "    ).assign(cp_flag=flag,\n",
    "             strike=lambda d: d[\"var\"].str.replace(prefix,\"\").astype(int)).drop(columns=\"var\")\n",
    "\n",
    "    tl = pd.concat([\n",
    "        melt(test_wide, [c for c in test_wide.columns if c.startswith(\"call_iv_\")],0,\"call_iv_\"),\n",
    "        melt(test_wide, [c for c in test_wide.columns if c.startswith(\"put_iv_\") ],1,\"put_iv_\")\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    # ----- SVI baseline -----\n",
    "    tl[\"iv_svi\"] = np.nan\n",
    "    for key, sub in tqdm(tl.groupby([\"timestamp\",\"expiry\",\"underlying\"], sort=False),\n",
    "                         total=tl.groupby([\"timestamp\",\"expiry\",\"underlying\"]).ngroups,\n",
    "                         desc=\"SVI (test)\"):\n",
    "        p = params_dict.get(key)\n",
    "        tl.loc[sub.index,\"iv_svi\"] = svi_iv(p, sub[\"strike\"].values, key[2])\n",
    "\n",
    "    # ----- same features -----\n",
    "    tl[\"log_mny\"] = np.log(tl[\"strike\"]/tl[\"underlying\"])\n",
    "    tl[\"abs_mny\"] = tl[\"log_mny\"].abs()\n",
    "    tl[\"log_mny_sq\"] = tl[\"log_mny\"]**2\n",
    "    tl[\"log_mny_cu\"] = tl[\"log_mny\"]**3\n",
    "    tl[\"abs_strike_minus_underlying\"] = (tl[\"strike\"]-tl[\"underlying\"]).abs()\n",
    "    tl[\"cpflag_x_absmny\"] = tl[\"cp_flag\"]*tl[\"abs_mny\"]\n",
    "\n",
    "    X_cat = tl[base_feats].copy()\n",
    "    X_cat[\"cp_flag\"] = X_cat[\"cp_flag\"].astype(int)\n",
    "    Zcb = sum(m.predict(X_cat) for m in cat_models)/len(cat_models)\n",
    "\n",
    "    X_lgb = X_cat.copy(); X_lgb[\"Z_cb\"] = Zcb\n",
    "    Zlgb = sum(m.predict(X_lgb, num_iteration=m.best_iteration) for m in lgb_models)/len(lgb_models)\n",
    "\n",
    "    tl[\"iv_pred\"] = tl[\"iv_svi\"] + Zlgb\n",
    "\n",
    "    # ----- pivot back -----\n",
    "    out = test_wide.copy()\n",
    "    for (ts,exp,und), sub in tqdm(tl.groupby([\"timestamp\",\"expiry\",\"underlying\"], sort=False),\n",
    "                                  desc=\"pivot\"):\n",
    "        m = sub.assign(lbl=sub[\"cp_flag\"].map({0:\"call_iv_\",1:\"put_iv_\"})+sub[\"strike\"].astype(str))\n",
    "        out.loc[(out[\"timestamp\"]==ts)&(out[\"expiry\"]==exp)&(out[\"underlying\"]==und),\n",
    "                m[\"lbl\"]] = m[\"iv_pred\"].values\n",
    "\n",
    "    out.to_csv(out_csv, index=False)\n",
    "    print(\"✓ submission saved ->\", out_csv)\n",
    "\n",
    "# ===============================================================\n",
    "# 7. Example usage\n",
    "# ===============================================================\n",
    "# predict_iv_for_test(\"/kaggle/input/nk-iv-prediction/test_data.parquet\",\n",
    "#                     \"submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T17:59:23.298397Z",
     "iopub.status.busy": "2025-06-02T17:59:23.298039Z",
     "iopub.status.idle": "2025-06-02T17:59:23.303414Z",
     "shell.execute_reply": "2025-06-02T17:59:23.302270Z",
     "shell.execute_reply.started": "2025-06-02T17:59:23.298370Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "TRAINING_DATA = \"/kaggle/input/nk-iv-prediction/train_data.parquet\"\n",
    "TESTING_DATA = \"/kaggle/input/nk-iv-prediction/test_data.parquet\"\n",
    "SAMPLE_SUBMISSION_DATA = \"/kaggle/input/nk-iv-prediction/sample_submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T17:59:29.487503Z",
     "iopub.status.busy": "2025-06-02T17:59:29.487196Z",
     "iopub.status.idle": "2025-06-02T17:59:29.500894Z",
     "shell.execute_reply": "2025-06-02T17:59:29.499753Z",
     "shell.execute_reply.started": "2025-06-02T17:59:29.487479Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.optimize as opt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────\n",
    "# 1.1  Raw SVI total‐variance function and objective\n",
    "# ─────────────────────────────────────────────────────────────────\n",
    "\n",
    "def svi_total_variance(params, k_vals):\n",
    "    \"\"\"\n",
    "    Raw SVI total variance:\n",
    "        w(k) = a + b * [ rho*(k - m) + sqrt((k - m)^2 + sigma^2 ) ].\n",
    "    params = [a, b, rho, m, sigma]\n",
    "    k_vals = array of log-moneyness\n",
    "    Returns array w(k_vals)\n",
    "    \"\"\"\n",
    "    a, b, rho, m, sig = params\n",
    "    return a + b * (rho * (k_vals - m) + np.sqrt((k_vals - m)**2 + sig**2))\n",
    "\n",
    "\n",
    "def svi_obj_on_iv(params, k_obs, iv_obs):\n",
    "    \"\"\"\n",
    "    Objective (sum of squared errors on IV) for a single row:\n",
    "       minimize Σ_i [ iv_obs_i - sqrt( w(k_i) ) ]^2,\n",
    "    where w(k) = svi_total_variance(params, k).\n",
    "    \"\"\"\n",
    "    w = svi_total_variance(params, k_obs)\n",
    "    # enforce positive total variance\n",
    "    if np.any(w <= 0):\n",
    "        return np.inf\n",
    "    iv_model = np.sqrt(w)\n",
    "    return np.sum((iv_model - iv_obs)**2)\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────\n",
    "# 1.2  Per‐row SVI calibration (calls OR puts)\n",
    "# ─────────────────────────────────────────────────────────────────\n",
    "\n",
    "def calibrate_svi_for_row(strikes, ivs, underlying):\n",
    "    \"\"\"\n",
    "    Fit SVI for a *single row*'s subset of strikes & IVs.\n",
    "    Inputs:\n",
    "      - strikes:  array of strike prices where IV is known\n",
    "      - ivs:      array of corresponding observed IVs\n",
    "      - underlying: scalar underlying price for that row\n",
    "    Returns:\n",
    "      params (a,b,rho,m,sigma) if fit succeeds; else None\n",
    "    \"\"\"\n",
    "    # If too few points, bail out\n",
    "    if len(strikes) < 5:\n",
    "        return None\n",
    "\n",
    "    # Compute log-moneyness k_i = ln(K / underlying)\n",
    "    k_obs = np.log(strikes / underlying)\n",
    "    iv_obs = ivs.astype(float)\n",
    "\n",
    "    # Initial guess for [a,b,rho,m,sigma]\n",
    "    # - a0: roughly minimum total variance = (min iv)^2\n",
    "    v_min = np.min(iv_obs**2)\n",
    "    a0 = max(v_min * 0.8, 1e-6)  # a small positive floor\n",
    "    b0 = 0.1                     # small positive slope\n",
    "    rho0 = 0.0\n",
    "    m0 = 0.0                     # assume roughly ATM center\n",
    "    sig0 = 0.1\n",
    "    x0 = np.array([a0, b0, rho0, m0, sig0], dtype=float)\n",
    "\n",
    "    # Bounds: a>0, b>0, |rho|<0.999, m in [min(k)-0.5, max(k)+0.5], sigma>0\n",
    "    bounds = [\n",
    "        (1e-8, np.inf),                    # a\n",
    "        (1e-8, np.inf),                    # b\n",
    "        (-0.999, 0.999),                   # rho\n",
    "        (np.min(k_obs)-0.5, np.max(k_obs)+0.5),  # m\n",
    "        (1e-8, 5.0)                        # sigma\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        res = opt.minimize(\n",
    "            fun=lambda x: svi_obj_on_iv(x, k_obs, iv_obs),\n",
    "            x0=x0,\n",
    "            bounds=bounds,\n",
    "            method=\"L-BFGS-B\",\n",
    "            options={\"maxiter\":200, \"ftol\":1e-8}\n",
    "        )\n",
    "        if (not res.success) or np.any(res.x < 0):\n",
    "            return None\n",
    "        return res.x  # fitted [a,b,rho,m,sigma]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def fill_svi_smile(params, target_strikes, underlying):\n",
    "    \"\"\"\n",
    "    Given fitted SVI params = [a,b,rho,m,sigma], return IVs at target_strikes:\n",
    "      iv_pred = sqrt( w(k) )  where  k = ln(K/underlying).\n",
    "    If params is None, returns NaNs.\n",
    "    \"\"\"\n",
    "    if params is None:\n",
    "        return np.full(len(target_strikes), np.nan)\n",
    "    k_all = np.log(target_strikes / underlying)\n",
    "    w_all = svi_total_variance(params, k_all)\n",
    "    # ensure no negative variances\n",
    "    w_all = np.maximum(w_all, 0.0)\n",
    "    return np.sqrt(w_all)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Test Data & Sample Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T17:59:38.623909Z",
     "iopub.status.busy": "2025-06-02T17:59:38.623564Z",
     "iopub.status.idle": "2025-06-02T17:59:38.777259Z",
     "shell.execute_reply": "2025-06-02T17:59:38.776295Z",
     "shell.execute_reply.started": "2025-06-02T17:59:38.623882Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 2.1  Which columns correspond to calls and puts?\n",
    "#     We will need to parse the column names like \"call_iv_26000\" → strike=26000.\n",
    "\n",
    "test_df = pd.read_parquet(TESTING_DATA)\n",
    "sample_sub = pd.read_csv(SAMPLE_SUBMISSION_DATA)\n",
    "\n",
    "# Extract the full list of IV‐columns (they appear in sample_submission in order).\n",
    "iv_columns = [c for c in sample_sub.columns if (\"call_iv_\" in c) or (\"put_iv_\" in c)]\n",
    "\n",
    "# For convenience, build separate sorted lists of all possible strikes:\n",
    "call_cols = [c for c in iv_columns if c.startswith(\"call_iv_\")]\n",
    "put_cols  = [c for c in iv_columns if c.startswith(\"put_iv_\")]\n",
    "\n",
    "# Extract strike values as integers:\n",
    "call_strikes = sorted(int(c.replace(\"call_iv_\",\"\")) for c in call_cols)\n",
    "put_strikes  = sorted(int(c.replace(\"put_iv_\",\"\")) for c in put_cols)\n",
    "\n",
    "# Map column names → strike (dict):\n",
    "call_col_to_strike = {f\"call_iv_{k}\": k for k in call_strikes}\n",
    "put_col_to_strike  = {f\"put_iv_{k}\":  k for k in put_strikes}\n",
    "\n",
    "print(\"Total test rows:\", len(test_df))\n",
    "print(\"Number of call strikes:\", len(call_strikes), \"→\", call_strikes[:5], \"…\", call_strikes[-5:])\n",
    "print(\"Number of put strikes: \", len(put_strikes),  \"→\", put_strikes[:5],  \"…\", put_strikes[-5:])\n",
    "print(sample_sub.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Row-wise Imputation Loop (calls + puts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-06-02T17:59:42.022258Z",
     "iopub.status.busy": "2025-06-02T17:59:42.021909Z",
     "iopub.status.idle": "2025-06-02T18:17:42.203763Z",
     "shell.execute_reply": "2025-06-02T18:17:42.202921Z",
     "shell.execute_reply.started": "2025-06-02T17:59:42.022231Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Prepare an output array of shape (n_rows, n_iv_columns)\n",
    "n_rows = len(test_df)\n",
    "n_ivcols = len(iv_columns)\n",
    "\n",
    "# We'll build a DataFrame exactly matching sample_sub’s shape/order:\n",
    "out = pd.DataFrame(index=np.arange(n_rows), columns=sample_sub.columns, dtype=float)\n",
    "out[\"timestamp\"] = sample_sub[\"timestamp\"].values\n",
    "\n",
    "# To show progress:\n",
    "pbar = tqdm(total=n_rows, desc=\"Imputing rows\")\n",
    "\n",
    "for i, row in test_df.iterrows():\n",
    "    underlying = float(row[\"underlying\"])  # NIFTY50 spot price\n",
    "    \n",
    "    # ────────────────────────────────────────────\n",
    "    # 3.1  Calls: gather observed call IVs for this row\n",
    "    # ────────────────────────────────────────────\n",
    "    observed_call_strikes = []\n",
    "    observed_call_ivs     = []\n",
    "    missing_call_strikes  = []\n",
    "    \n",
    "    for colname, strike in call_col_to_strike.items():\n",
    "        val = row[colname]\n",
    "        if np.isfinite(val):\n",
    "            observed_call_strikes.append(strike)\n",
    "            observed_call_ivs.append(val)\n",
    "        else:\n",
    "            missing_call_strikes.append(strike)\n",
    "    \n",
    "    observed_call_strikes = np.array(observed_call_strikes, dtype=float)\n",
    "    observed_call_ivs     = np.array(observed_call_ivs, dtype=float)\n",
    "    missing_call_strikes  = np.array(missing_call_strikes, dtype=float)\n",
    "    \n",
    "    # Fit SVI on calls\n",
    "    params_call = calibrate_svi_for_row(observed_call_strikes, observed_call_ivs, underlying)\n",
    "    \n",
    "    # Predict missing calls\n",
    "    if params_call is not None:\n",
    "        call_preds = fill_svi_smile(params_call, missing_call_strikes, underlying)\n",
    "    else:\n",
    "        # Fallback: simple linear interpolation in k‐space\n",
    "        if len(observed_call_strikes) >= 2:\n",
    "            k_obs = np.log(observed_call_strikes / underlying)\n",
    "            iv_obs = observed_call_ivs\n",
    "            k_miss = np.log(missing_call_strikes / underlying)\n",
    "            # np.interp on k_miss (left/right are filled with nearest)\n",
    "            iv_pred = np.interp(k_miss, k_obs, iv_obs)\n",
    "            call_preds = iv_pred\n",
    "        else:\n",
    "            # Too few points → fill with a constant (e.g. nearest neighbor)\n",
    "            call_preds = np.full(len(missing_call_strikes), observed_call_ivs.mean() if len(observed_call_ivs)>0 else 0.2)\n",
    "    \n",
    "    # Write back into output DataFrame\n",
    "    #   - For observed indices: keep the original value\n",
    "    for colname, strike in call_col_to_strike.items():\n",
    "        if not np.isfinite(row[colname]):\n",
    "            # find index in missing_call_strikes\n",
    "            idx = np.where(missing_call_strikes == strike)[0][0]\n",
    "            out.at[i, colname] = call_preds[idx]\n",
    "        else:\n",
    "            out.at[i, colname] = row[colname]\n",
    "    \n",
    "    # ────────────────────────────────────────────\n",
    "    # 3.2  Puts: gather observed put IVs for this row\n",
    "    # ────────────────────────────────────────────\n",
    "    observed_put_strikes = []\n",
    "    observed_put_ivs     = []\n",
    "    missing_put_strikes  = []\n",
    "    \n",
    "    for colname, strike in put_col_to_strike.items():\n",
    "        val = row[colname]\n",
    "        if np.isfinite(val):\n",
    "            observed_put_strikes.append(strike)\n",
    "            observed_put_ivs.append(val)\n",
    "        else:\n",
    "            missing_put_strikes.append(strike)\n",
    "    \n",
    "    observed_put_strikes = np.array(observed_put_strikes, dtype=float)\n",
    "    observed_put_ivs     = np.array(observed_put_ivs, dtype=float)\n",
    "    missing_put_strikes  = np.array(missing_put_strikes, dtype=float)\n",
    "    \n",
    "    # Fit SVI on puts\n",
    "    params_put = calibrate_svi_for_row(observed_put_strikes, observed_put_ivs, underlying)\n",
    "    \n",
    "    # Predict missing puts\n",
    "    if params_put is not None:\n",
    "        put_preds = fill_svi_smile(params_put, missing_put_strikes, underlying)\n",
    "    else:\n",
    "        # Fallback: linear interpolation in k‐space\n",
    "        if len(observed_put_strikes) >= 2:\n",
    "            k_obs = np.log(observed_put_strikes / underlying)\n",
    "            iv_obs = observed_put_ivs\n",
    "            k_miss = np.log(missing_put_strikes / underlying)\n",
    "            iv_pred = np.interp(k_miss, k_obs, iv_obs)\n",
    "            put_preds = iv_pred\n",
    "        else:\n",
    "            put_preds = np.full(len(missing_put_strikes), observed_put_ivs.mean() if len(observed_put_ivs)>0 else 0.2)\n",
    "    \n",
    "    # Write back into output DataFrame\n",
    "    for colname, strike in put_col_to_strike.items():\n",
    "        if not np.isfinite(row[colname]):\n",
    "            idx = np.where(missing_put_strikes == strike)[0][0]\n",
    "            out.at[i, colname] = put_preds[idx]\n",
    "        else:\n",
    "            out.at[i, colname] = row[colname]\n",
    "    \n",
    "    pbar.update(1)\n",
    "    print(\"imputed row \",i)\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sanity‐check & Export Submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T18:18:12.647501Z",
     "iopub.status.busy": "2025-06-02T18:18:12.647203Z",
     "iopub.status.idle": "2025-06-02T18:18:13.614192Z",
     "shell.execute_reply": "2025-06-02T18:18:13.613180Z",
     "shell.execute_reply.started": "2025-06-02T18:18:12.647473Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 4.1  Quick sanity: check no NaNs remain\n",
    "assert out.isna().sum().sum() == 0, \"There are still NaNs in the output!\"\n",
    "\n",
    "# 4.2  Make sure the column order matches sample_submission\n",
    "out = out[sample_sub.columns]\n",
    "\n",
    "# 4.3  Write submission.csv\n",
    "out.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Wrote submission.csv with shape\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T19:24:27.988118Z",
     "iopub.status.busy": "2025-06-02T19:24:27.987383Z",
     "iopub.status.idle": "2025-06-02T19:24:28.415586Z",
     "shell.execute_reply": "2025-06-02T19:24:28.414544Z",
     "shell.execute_reply.started": "2025-06-02T19:24:27.988068Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed‐cell RMSE (should be 0): 0.000000000000\n",
      "✔ Sanity check passed: shape, columns, no NaNs, observed IVs untouched, IV range OK.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 6.1  Read the sample submission (with timestamp + IV columns) and our submission\n",
    "sample_sub = pd.read_csv(SAMPLE_SUBMISSION_DATA)\n",
    "sub        = pd.read_csv(\"submission.csv\")\n",
    "\n",
    "# 6.2  Check that shape and column order match exactly\n",
    "assert sub.shape == sample_sub.shape, f\"Shape mismatch: {sub.shape} vs {sample_sub.shape}\"\n",
    "assert list(sub.columns) == list(sample_sub.columns), \"Column order or names mismatch!\"\n",
    "\n",
    "# 6.3  Check for any NaNs in our submission\n",
    "assert sub.isna().sum().sum() == 0, \"Submission has NaNs!\"\n",
    "\n",
    "# 6.4  Extract the list of IV columns (everything except 'timestamp')\n",
    "iv_columns = [c for c in sub.columns if c != \"timestamp\"]\n",
    "\n",
    "# 6.5  Confirm that for every IV column, any originally‐observed IV in test_df was preserved exactly\n",
    "#      → We need test_df loaded in memory (with the same ordering as sub)\n",
    "test_df = pd.read_parquet(TESTING_DATA)\n",
    "\n",
    "# Mask of “observed” IV entries in test_df\n",
    "mask_obs = test_df[iv_columns].notna()\n",
    "\n",
    "# Compare only those entries:\n",
    "pred_vals = sub[iv_columns].values\n",
    "true_vals = test_df[iv_columns].values\n",
    "\n",
    "diff = pred_vals[mask_obs.values] - true_vals[mask_obs.values]\n",
    "assert np.allclose(diff, 0.0, atol=1e-12), \"Some observed IVs were changed!\"\n",
    "\n",
    "# 6.6  Compute and print RMSE over **only** the observed cells:\n",
    "rmse_observed = np.sqrt(np.mean((diff) ** 2))\n",
    "print(f\"Observed‐cell RMSE (should be 0): {rmse_observed:.12f}\")\n",
    "\n",
    "# 6.7  Compute and print RMSE over the **filled** (formerly‐masked) cells:\n",
    "mask_filled = ~mask_obs.values  # True where test_df was NaN\n",
    "filled_diff = pred_vals[mask_filled]  # no ground truth for these, so we skip\n",
    "# We cannot compute “true” for masked cells (they’re NaN), so we skip RMSE here.\n",
    "# If you have a hold‐out on train, compute that separately.\n",
    "\n",
    "# 6.8  Spot‐check that all IVs are within a reasonable range [0, 5]:\n",
    "iv_data = sub[iv_columns].values.flatten()\n",
    "assert (iv_data >= 0.0).all() and (iv_data <= 5.0).all(), \"Some IVs are out of a reasonable range!\"\n",
    "\n",
    "print(\"✔ Sanity check passed: shape, columns, no NaNs, observed IVs untouched, IV range OK.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flattened long-form transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T19:25:48.293858Z",
     "iopub.status.busy": "2025-06-02T19:25:48.293511Z",
     "iopub.status.idle": "2025-06-02T19:26:10.747260Z",
     "shell.execute_reply": "2025-06-02T19:26:10.746235Z",
     "shell.execute_reply.started": "2025-06-02T19:25:48.293827Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long‐form train shape: (9273680, 48)\n",
      "             timestamp  underlying      expiry        X0        X1  \\\n",
      "0  1745296089000000000     24160.9  2025-04-24 -0.092103 -0.084458   \n",
      "1  1745304077000000000     24188.1  2025-04-24 -0.013699 -0.023263   \n",
      "2  1745313495000000000     24148.6  2025-04-24 -0.395427 -0.056440   \n",
      "3  1745313499000000000     24147.4  2025-04-24  0.007829 -0.086614   \n",
      "4  1745313608000000000     24155.9  2025-04-24  0.012404 -0.005619   \n",
      "\n",
      "             X2        X3            X4        X5        X6  ...       X35  \\\n",
      "0  1.025842e+05  0.001655 -1.379624e+06  0.027959 -0.020240  ...  0.024715   \n",
      "1  1.086423e+05 -0.004734  2.528508e+06 -0.006439 -0.011416  ... -0.004020   \n",
      "2 -1.194717e+06  0.005011 -1.185146e+08 -0.027625 -0.002189  ... -0.035342   \n",
      "3  5.936540e+05  0.001347  3.826919e+06 -0.052881 -0.015226  ... -0.045472   \n",
      "4  2.419948e+05 -0.001172  4.596446e+06  0.039326  0.015934  ...  0.044814   \n",
      "\n",
      "        X36       X37           X38           X39            X40  \\\n",
      "0  0.530894 -0.002354 -3.224848e+05 -1.600795e+06   13063.446970   \n",
      "1 -1.429919 -0.000843  1.658073e+06 -1.742468e+06   31364.628427   \n",
      "2 -0.523109  0.013778 -2.646675e+06 -5.051008e+07 -847564.971737   \n",
      "3  0.741664  0.002590 -1.607321e+06  4.170899e+05  333918.361928   \n",
      "4 -0.015472  0.012185  6.820360e+05  4.081106e+06    3309.895833   \n",
      "\n",
      "             X41        iv  cp_flag  strike  \n",
      "0  445511.363636  0.237872        0   23500  \n",
      "1  -46123.161765  0.236015        0   23500  \n",
      "2 -225333.881579  0.225757        0   23500  \n",
      "3 -114960.453869  0.220805        0   23500  \n",
      "4  183946.289063  0.220088        0   23500  \n",
      "\n",
      "[5 rows x 48 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Read the original wide-format train.parquet\n",
    "train = pd.read_parquet(TRAINING_DATA)\n",
    "\n",
    "# 2. Identify all IV columns for calls and for puts\n",
    "iv_cols = [c for c in train.columns if c.startswith(\"call_iv_\") or c.startswith(\"put_iv_\")]\n",
    "call_cols = [c for c in iv_cols if c.startswith(\"call_iv_\")]\n",
    "put_cols  = [c for c in iv_cols if c.startswith(\"put_iv_\")]\n",
    "\n",
    "# 3. Melt calls into long form:\n",
    "#    Each row: timestamp, underlying, expiry, X0–X41, var=\"call_iv_K\", iv=value, cp_flag=0, strike=K\n",
    "calls_long = train.melt(\n",
    "    id_vars=[\"timestamp\", \"underlying\", \"expiry\"] + [f\"X{i}\" for i in range(42)],\n",
    "    value_vars=call_cols,\n",
    "    var_name=\"var\", value_name=\"iv\"\n",
    ")\n",
    "calls_long[\"cp_flag\"] = 0  # calls\n",
    "calls_long[\"strike\"] = calls_long[\"var\"].str.replace(\"call_iv_\", \"\").astype(int)\n",
    "calls_long = calls_long.drop(columns=\"var\")\n",
    "\n",
    "# 4. Melt puts into long form similarly:\n",
    "puts_long = train.melt(\n",
    "    id_vars=[\"timestamp\", \"underlying\", \"expiry\"] + [f\"X{i}\" for i in range(42)],\n",
    "    value_vars=put_cols,\n",
    "    var_name=\"var\", value_name=\"iv\"\n",
    ")\n",
    "puts_long[\"cp_flag\"] = 1  # puts\n",
    "puts_long[\"strike\"] = puts_long[\"var\"].str.replace(\"put_iv_\", \"\").astype(int)\n",
    "puts_long = puts_long.drop(columns=\"var\")\n",
    "\n",
    "# 5. Concatenate calls_long and puts_long into a single DataFrame\n",
    "train_long = pd.concat([calls_long, puts_long], ignore_index=True)\n",
    "\n",
    "# 6. Reset index and inspect\n",
    "train_long = train_long.reset_index(drop=True)\n",
    "print(\"Long‐form train shape:\", train_long.shape)\n",
    "print(train_long.head())\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12520411,
     "sourceId": 104024,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
